{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lavender/02_SaleChatbot/venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import SpladeEncoder\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "FIREWORKS_API_KEY = os.getenv(\"FIREWORKS_API_KEY\")\n",
    "INDEX_NAME = \"salechatbot\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "splade = SpladeEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/clip-ViT-B-32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import (\n",
    "    PineconeHybridSearchRetriever,\n",
    ")\n",
    "\n",
    "retriever = PineconeHybridSearchRetriever(\n",
    "    embeddings=embeddings, sparse_encoder=splade, index=index, top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a sale assistant. \n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", max_retries=3, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫°n c√≥ th·ªÉ xem x√©t b√†n ph√≠m gaming Tech77 v·ªõi gi√° 620.000 ƒë·ªìng ho·∫∑c b√†n ph√≠m game Fuhlen L411 v·ªõi gi√° 200.000 ƒë·ªìng. C·∫£ hai ƒë·ªÅu c√≥ s·∫µn. Thanks for asking!"
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"T√¥i mu·ªën mua b√†n ph√≠m gaming.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = retriever.invoke(\"T√¥i mu·ªën mua b√†n ph√≠m gaming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B√†n Ph√≠m C∆° Gaming d√¢y usb GK102 Hotswap ch·ªëng ·ªìn cho m√°y t√≠nh laptop h√†ng nh·∫≠p kh·∫©u\n",
      "B√†n Ph√≠m Game Fuhlen L411 - H√†ng Ch√≠nh H√£ng\n",
      "B√†n ph√≠m gi·∫£ c∆° K518 k√®m chu·ªôt, b√†n ph√≠m m√°y t√≠nh ch·ªëng n∆∞·ªõc led rgb ch∆°i game l√†m vi·ªác vƒÉn ph√≤ng cho PC laptop - h√†ng ch√≠nh h√£ng\n",
      "B·ªô B√†n Ph√≠m Chu·ªôt Kh√¥ng D√¢y Besti BTY01 Nh·ªè G·ªçn, Ti·ªán L·ª£i Mang Theo- H√†ng Ch√≠nh H√£ng\n",
      "Mi·∫øng d√°n b√†n ph√≠m ti·∫øng H√†n Qu·ªëc H√†ng Nh·∫≠p Kh·∫©u\n",
      "Bao da Book cover b√†n ph√≠m d√†nh cho samsung Tab S7+/TAB S7 FE/TAB S8+ SLIM EF-DT730 H√†ng ch√≠nh h√£ng\n",
      "Cu·ªôn m√∫t x·ªëp nhi·ªÅu m√†u b·ªçc g√≥c b√†n ch·ªØ U bo g√≥c b·∫£o v·ªá em b√© tr√°nh va ch·∫°m tr·∫ßy x∆∞·ªõc Legaxi\n",
      "B√†n ph√≠m c∆° kh√¥ng d√¢y Newmen GM610 - H√†ng ch√≠nh h√£ng\n",
      "B√†n ph√≠m Magic Keyboard Apple MK2A3 (US keyboard)\n",
      "ƒê·∫ßu Android TVbox Mytv net Phi√™n b·∫£n Ram 2G/16G 4G/32G ƒëi·ªÅu khi·ªÉn IR- Xem 200 K√™nh truy·ªÅn h√¨nh mi·ªÖn ph√≠ - H√†ng Ch√≠nh H√£ng\n"
     ]
    }
   ],
   "source": [
    "_=[print(example.metadata[\"name\"]) for example in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     user_input = input(\"User\")\n",
    "#     print(\"User:\", user_input)\n",
    "#     if user_input in [\"q\", \"quit\", \"bye\"]:\n",
    "#         break\n",
    "\n",
    "#     print(\"Chatbot:\", end=\" \", flush=True)\n",
    "#     for chunk in rag_chain.stream(user_input):\n",
    "#         print(chunk, end=\"\", flush=True)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
